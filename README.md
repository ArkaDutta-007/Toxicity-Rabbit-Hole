# Toxicity-Rabbit-Hole-Dataset

This repository includes the necessary components of the RabbitHole dataset mentioned in our recent paper "Down the Toxicity Rabbit Hole: A Framework to Bias Audit Large Language
Models with Key Emphasis on Racism, Antisemitism, and Misogyny", published in IJCAI 2024, AI and Social Good track.

Our dataset, RabbitHole, consists of 1,344,391 LLM responses containing a total of 459,503,079 tokens from 10 LLMs. 
We consider 1,266 identity groups with a vision toward safety for all. We consider 193 countries recognized by the United Nations. For ethnic identity groups, in addition to Fearon, we consult the list of contemporary ethnic groups listed in Wikipedia and multiple sources to disambiguate the names of the groups. Overall, we obtain 1,023 ethnic groups. We prompt PaLM 2 to suggest a list of 50 religions. This list contains all religions followed by a large group of people. 

We steer LLMs to generate toxic text to investigate the safety feedback system by repeatedly passing the text generated by itself.  Also, RabbitHole contains substantially more content indicating physical harm than any other existing datasets for this purpose.

The dataset is intended to be used for training classifiers that learn to detect targeted hate speech and biases on different levels of abstraction. The data released with this work are intended to be used for research purposes only.

WARNING: This repository contains and discusses content that is offensive or upsetting. All materials are intended to support research that improves toxicity detection methods. Included examples of toxicity do not represent how the authors or sponsors feel about any identity groups.


## Downloading the Dataset

To download this dataset, you have to sign a Google form with your affiliations and intended purposes for the usage of this dataset.


