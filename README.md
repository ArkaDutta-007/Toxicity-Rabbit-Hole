# Toxicity-Rabbit-Hole-Dataset

This is

Overall, our dataset, \texttt{RabbitHole}, consists of 1,344,391 LLM responses from 10 LLMs. Upon acceptance, we will make this dataset publicly available along with the model snapshots that generated them. Through an adversarial game between the LLM and a toxicity classifier, Hartvigsen \textit{et al.} \shortcite{hartvigsen2022toxigen} present a large-scale classification dataset dubbed \texttt{Toxigen} on implicit toxicity. This setup allows the model to generate toxic text that the classifier is unable to detect. In contrast, we steer \texttt{PaLM 2} to generate toxic text to investigate the safety feedback system by repeatedly passing the text generated by itself. \texttt{Toxigen} considers a small set of target groups and focuses on implicit hate. In contrast, we consider 1,266 identity groups with a vision toward safety for all. Also, \texttt{RabbitHole} contains substantially more content indicating physical harm (see, 
